{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMS Spam Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Label                                               Text  encoded_label\n",
      "0   ham  Go until jurong point, crazy.. Available only ...              0\n",
      "1   ham                      Ok lar... Joking wif u oni...              0\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...              1\n",
      "3   ham  U dun say so early hor... U c already then say...              0\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...              0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read in data from csv\n",
    "data = pd.read_csv(\"datasets/spam.csv\", encoding='latin-1')\n",
    "\n",
    "# Drop columns with NaN values\n",
    "for col in data.columns:\n",
    "    if 'Unnamed' in col:\n",
    "        data.drop(columns=col, inplace=True)\n",
    "\n",
    "# Rename columns to useful names\n",
    "data = data.rename(columns={\"v1\":\"Label\", \"v2\":\"Text\"})\n",
    "\n",
    "# Add an encoding for the labels: ham = 0, spam = 1\n",
    "enc_labels = []\n",
    "for item in data[\"Label\"]:\n",
    "    enc_labels.append(0 if item == \"ham\" else 1)\n",
    "data.insert(loc=2, column=\"encoded_label\", value=enc_labels)\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the average number of tokens in all sentences\n",
    "sentences = data[\"Text\"].values\n",
    "\n",
    "avg_sentence_len = 0\n",
    "for sentence in sentences:\n",
    "    avg_sentence_len += len(sentence.split()) # adding the number of words in each sentence\n",
    "avg_sentence_len = avg_sentence_len // len(sentences) # divide by the total number of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15585\n"
     ]
    }
   ],
   "source": [
    "# Find the total number of unique words\n",
    "s = set()\n",
    "for sentence in sentences:\n",
    "    for word in sentence.split():\n",
    "        s.add(word)\n",
    "num_unique_words = len(s)\n",
    "print(num_unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[\"Text\"], data[\"encoded_label\"], train_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'keras.api._v2.keras.activations' has no attribute 'relu6'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[73], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m x \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mFlatten()(x)\n\u001b[0;32m     16\u001b[0m x \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDropout(\u001b[38;5;241m0.5\u001b[39m)(x)\n\u001b[1;32m---> 17\u001b[0m x \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m32\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivations\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu6\u001b[49m, kernel_regularizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mregularizers\u001b[38;5;241m.\u001b[39ml2(\u001b[38;5;241m1e-4\u001b[39m))(x)\n\u001b[0;32m     18\u001b[0m x \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDropout(\u001b[38;5;241m0.5\u001b[39m)(x)\n\u001b[0;32m     19\u001b[0m output_layer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m1\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m\"\u001b[39m)(x)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'keras.api._v2.keras.activations' has no attribute 'relu6'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "text_vec = tf.keras.layers.TextVectorization(max_tokens=num_unique_words, standardize=\"lower_and_strip_punctuation\", \n",
    "                                       output_sequence_length=avg_sentence_len, output_mode=\"int\")\n",
    "text_vec.adapt(X_train)\n",
    "embed_layer = tf.keras.layers.Embedding(input_dim=num_unique_words, output_dim=128, input_length=avg_sentence_len,\n",
    "                                        embeddings_initializer=\"uniform\")\n",
    "\n",
    "input_layer = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n",
    "vec_layer = text_vec(input_layer)\n",
    "embedding_layer = embed_layer(vec_layer)\n",
    "# x = tf.keras.layers.Dropout(0.75)(embedding_layer)\n",
    "x = tf.keras.layers.GlobalAveragePooling1D()(embedding_layer)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "x = tf.keras.layers.Dense(32, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(1e-4))(x)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "output_layer = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 1s 4ms/step\n",
      "[0 0 0 ... 0 0 0]\n",
      "Accuracy: 0.8654708520179372\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daniel Cruz\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred = y_pred.astype(int)\n",
    "\n",
    "y_pred1D = []\n",
    "for i in range(len(y_pred)):\n",
    "    y_pred1D.append(y_pred[i][0])\n",
    "\n",
    "y_pred1D = pd.Series(y_pred1D)\n",
    "\n",
    "print(y_pred1D.values)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_pred=y_pred1D, y_true=y_test))\n",
    "print(\"Precision:\", precision_score(y_pred=y_pred1D, y_true=y_test))\n",
    "print(\"Recall:\", recall_score(y_pred=y_pred1D, y_true=y_test))\n",
    "print(\"F1:\", f1_score(y_pred=y_pred1D, y_true=y_test))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
